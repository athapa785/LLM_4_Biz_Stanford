{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4foG8bRBCLoWD+TzsB4lV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athapa785/LLM_4_Biz_Stanford/blob/main/aditya_thapa_llm4biz_homework_2_summarize_w_langchain_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Aditya Thapa](https://www.linkedin.com/in/aditya-thapa-56442b11b/)\n",
        "SLAC National Accelerator Laboratory | Stanford University"
      ],
      "metadata": {
        "id": "6Ljn4wqzPlfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2\n",
        "## LLM for Biz with Python\n",
        "Stanford Continuing Education\n",
        "TECH 16"
      ],
      "metadata": {
        "id": "5c0qCHBJQT3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project utilizes Large Language Models (LLMs), specifically OpenAI's GPT models, in the Google Colab environment. It employs the LangChain framework to enhance LLM capabilities for various Natural Language Processing (NLP) tasks, demonstrating its effectiveness for diverse applications.\n",
        "\n",
        "Key explorations include:\n",
        "\n",
        "- **Document summarization:** Summarizing the research paper \"Attention is All You Need\" by comparing LangChain's \"stuff\" and \"map_reduce\" methods with tailored prompts for refinement.\n",
        "- **Language translation:** Creating a translation tool to convert English text into multiple languages, providing both native script and roman transliteration, using LangChain's prompt engineering.\n",
        "- **Creative text generation:** Developing a pet name generator with LangChain to create unique names based on pet characteristics."
      ],
      "metadata": {
        "id": "yoNfIxA-QGZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 563,
      "metadata": {
        "id": "eHdqIWYd-o0D"
      },
      "outputs": [],
      "source": [
        "# Initialize key and client\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "\n",
        "client = OpenAI(api_key=open_ai_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab \"Attention is all you need\"\n",
        "!wget https://arxiv.org/pdf/1706.03762"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn__1JVEI6Ki",
        "outputId": "cf820d3b-04cb-4ff4-ffc3-3f98213d83af"
      },
      "execution_count": 564,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-08 06:17:31--  https://arxiv.org/pdf/1706.03762\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.67.42, 151.101.131.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.67.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2215244 (2.1M) [application/pdf]\n",
            "Saving to: ‘1706.03762.3’\n",
            "\n",
            "\r1706.03762.3          0%[                    ]       0  --.-KB/s               \r1706.03762.3        100%[===================>]   2.11M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-08 06:17:32 (172 MB/s) - ‘1706.03762.3’ saved [2215244/2215244]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U langchain-community pypdf langchain-openai"
      ],
      "metadata": {
        "id": "YIBLzgMkJvV3"
      },
      "execution_count": 565,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "qvxFe8W3Jy9U"
      },
      "execution_count": 566,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"1706.03762\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "CSDGO8yYJ4zR"
      },
      "execution_count": 567,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "rijmCHiAKNaN"
      },
      "execution_count": 568,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "tK1xsDyYKYJz"
      },
      "execution_count": 569,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4-turbo-preview\", api_key=open_ai_key)"
      ],
      "metadata": {
        "id": "w0JjjXcjKRjY"
      },
      "execution_count": 570,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain type: \"stuff\""
      ],
      "metadata": {
        "id": "CrN37WGXNjmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain_stuff = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "resp_stuff = chain_stuff.invoke(pages)"
      ],
      "metadata": {
        "id": "SzKa8QVOmJB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574d4ef4-8c1e-4ef0-b3b3-5308884e3595"
      },
      "execution_count": 572,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 54.8 ms, sys: 3.32 ms, total: 58.1 ms\n",
            "Wall time: 5.86 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(resp_stuff[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "3C-Eoy-aKhbC",
        "outputId": "7c2a0e7c-3261-4dda-f956-c55e5ba9989f"
      },
      "execution_count": 573,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper \"Attention Is All You Need\" by Ashish Vaswani and colleagues at Google introduces the Transformer, a novel neural network architecture that relies entirely on an attention mechanism, eliminating the need for recurrent or convolutional layers in sequence transduction models. This architecture allows for significantly more parallelization, reducing training time and improving performance on machine translation tasks. The Transformer achieves state-of-the-art results on the WMT 2014 English-to-German and English-to-French translation tasks, outperforming existing models and ensembles with less training time. The model's design includes multi-head self-attention, positional encodings, and a novel scaling method for the dot-product attention mechanism. The paper demonstrates the Transformer's effectiveness not only in translation but also in English constituency parsing, suggesting its potential applicability to a wide range of sequence modeling problems. The research contributes to the understanding of attention mechanisms and their central role in neural network architectures for natural language processing."
          },
          "metadata": {},
          "execution_count": 573
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain type: \"map_reduce\""
      ],
      "metadata": {
        "id": "4Ls4koKQNmjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "chain_map_reduce = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "resp_map_reduce = chain_map_reduce.invoke(pages)"
      ],
      "metadata": {
        "id": "fEC74rEHMNTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bdd692-8f79-4db5-8455-21792a52b874"
      },
      "execution_count": 574,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 680 ms, sys: 75.5 ms, total: 755 ms\n",
            "Wall time: 1min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noticed that map_reduce takes much longer."
      ],
      "metadata": {
        "id": "umlfn_DhlQ0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(resp_map_reduce[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "NbrcMCA-MS-C",
        "outputId": "1b9b0aa4-921a-4490-c93b-947784c12625"
      },
      "execution_count": 575,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper \"Attention Is All You Need\" by Ashish Vaswani et al. introduces the Transformer, a novel neural network architecture that exclusively uses attention mechanisms, eliminating recurrent and convolutional layers. This model achieves state-of-the-art performance in machine translation, notably on the WMT 2014 English-to-German and English-to-French tasks, and shows potential for generalization to other tasks like English constituency parsing. The Transformer's efficiency and performance are attributed to its unique structure, which allows for greater parallelization and includes innovations such as multi-head attention and positional encoding. This work, presented at NIPS 2017, marks a significant advancement in sequence modeling and has implications for future research in deep learning and natural language processing (NLP). Additionally, the document discusses broader advancements in NLP and deep learning from 2013 to 2017, covering developments in RNNs, CNNs, machine translation, and neural network optimization, reflecting the field's progression towards more sophisticated models for understanding human language."
          },
          "metadata": {},
          "execution_count": 575
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's try prompting the model to format the summary text per our preferences."
      ],
      "metadata": {
        "id": "GHLyP0AnAaOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "n6T0EYwAKoiu"
      },
      "execution_count": 576,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt template\n",
        "prompt_template = \"\"\"Write a concise summary in a maximum of 3 bullets of the following text enclosed within three backticks:\n",
        "```{text}```\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "JM7CNBViLeKy"
      },
      "execution_count": 592,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LLM chain\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "TtTwirsXLoWB"
      },
      "execution_count": 593,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Define StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
        "\n",
        "resp = stuff_chain.invoke(pages)"
      ],
      "metadata": {
        "id": "Omnqy13FLvhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b8f884-4e61-4425-b787-a63ac100cb42"
      },
      "execution_count": 594,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 70.2 ms, sys: 3.05 ms, total: 73.2 ms\n",
            "Wall time: 6.52 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(resp[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "ZbfZMYz4L57V",
        "outputId": "1f61d36c-6859-4883-d957-ab553d062dbe"
      },
      "execution_count": 595,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Google introduces the Transformer, a novel neural network architecture based solely on attention mechanisms, eliminating the need for recurrence and convolutions. This architecture achieves state-of-the-art results on the WMT 2014 English-to-German and English-to-French translation tasks, demonstrating superior quality, parallelizability, and training efficiency.\n- The Transformer model utilizes self-attention layers to compute representations of its input and output without relying on sequence-aligned RNNs or convolution, facilitating significantly more parallelization. It also incorporates multi-head attention to allow the model to focus on different positions of the input sequence simultaneously.\n- Experiments show that the Transformer model not only outperforms existing models in translation tasks but also generalizes well to other tasks like English constituency parsing, achieving competitive results with much less training time and computational resources."
          },
          "metadata": {},
          "execution_count": 595
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if we prompt the LLM to compare \"stuff\" and \"map_reduce\" outputs?"
      ],
      "metadata": {
        "id": "WtLH02mRGgpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_prompt_template = \"\"\"You are an analytical thinker.\n",
        "                            I will provide you two texts and you will compare the two texts.\n",
        "                            The first text is the output of the stuff chain and the second text is the output of the map_reduce chain.\n",
        "                            I want you to note down any differences between the style.\n",
        "                            Also, compare the amount of information in the two texts.\n",
        "                            Here are the texts:\n",
        "                            ```{text1}```\n",
        "                            ```{text2}```\n",
        "                            You shoudld refer to the texts as \"Stuff\" and \"Map Reduce\".\n",
        "                            Give a verdict of which chain is better.\n",
        "                          \"\"\"\n",
        "compare_prompt = PromptTemplate.from_template(compare_prompt_template)"
      ],
      "metadata": {
        "id": "NZDjUS8nGy_f"
      },
      "execution_count": 601,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain_compare = LLMChain(llm=llm, prompt=compare_prompt)"
      ],
      "metadata": {
        "id": "pGbnXWBHIFmz"
      },
      "execution_count": 602,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = resp_stuff[\"output_text\"]\n",
        "text2 = resp_map_reduce[\"output_text\"]"
      ],
      "metadata": {
        "id": "7aZ-Avp9I81a"
      },
      "execution_count": 603,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response_compare = llm_chain_compare.invoke({\"text1\": text1, \"text2\": text2})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej5ohJmsII_a",
        "outputId": "7abe1739-4b97-408e-b3aa-fc2e43b390f5"
      },
      "execution_count": 604,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 131 ms, sys: 22.4 ms, total: 154 ms\n",
            "Wall time: 20 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response_compare[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "MGkFpZn_JR3m",
        "outputId": "9b6e1d20-0268-48fa-bbfe-e92e6c212736"
      },
      "execution_count": 605,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Upon comparing the \"Stuff\" and \"Map Reduce\" texts, several differences in style and the amount of information presented become apparent.\n\n### Style Differences\n\n1. **Conciseness and Clarity**:\n   - The \"Stuff\" text is more detailed and explanatory, providing a comprehensive overview of the Transformer model, its components, and its achievements. It uses technical language appropriate for an audience familiar with neural network architectures.\n   - The \"Map Reduce\" text, while still informative, is more concise and slightly broader in its discussion. It mentions the Transformer's achievements and unique features but spends less time on each aspect. It also briefly touches on broader advancements in NLP and deep learning, providing context to the Transformer's development.\n\n2. **Technical Detail**:\n   - The \"Stuff\" text delves deeper into the specific components of the Transformer model, such as \"multi-head self-attention,\" \"positional encodings,\" and a \"novel scaling method.\" This detail is valuable for readers seeking a deeper understanding of the model's inner workings.\n   - The \"Map Reduce\" text mentions \"multi-head attention\" and \"positional encoding\" but does not elaborate on these components to the same extent. It also broadens the discussion to include the historical context of NLP and deep learning advancements.\n\n3. **Author Attribution**:\n   - The \"Stuff\" text attributes the paper to \"Ashish Vaswani and colleagues at Google,\" providing a clear picture of the authors' affiliations.\n   - The \"Map Reduce\" text uses \"Ashish Vaswani et al.,\" a more traditional academic citation style, which is concise but less informative about the authors' affiliations.\n\n### Amount of Information\n\n- **Depth of Coverage**:\n  - The \"Stuff\" text is more focused on the Transformer model itself, providing a detailed account of its features, performance, and potential applications. It is rich in specific details related to the model's architecture and achievements.\n  - The \"Map Reduce\" text, while covering the Transformer, also allocates space to discuss the broader context of NLP and deep learning advancements. This broader scope introduces additional information but at the expense of some depth regarding the Transformer model.\n\n- **Broader Context**:\n  - The \"Map Reduce\" text provides a broader historical context, mentioning the progression of NLP and deep learning from 2013 to 2017. This context is valuable for readers interested in the evolution of the field but may dilute the focus on the Transformer model itself.\n  - The \"Stuff\" text remains tightly focused on the Transformer, making it more suitable for readers primarily interested in the model and its direct implications.\n\n### Verdict\n\nThe choice between the \"Stuff\" and \"Map Reduce\" chains depends on the reader's needs:\n\n- If the goal is to gain a detailed understanding of the Transformer model, its architecture, and its specific contributions to NLP, the \"Stuff\" text is superior due to its depth and specificity.\n- If the goal is to understand the Transformer within the broader context of NLP and deep learning advancements, the \"Map Reduce\" text is better as it provides a concise overview of the model while also discussing related developments in the field.\n\nNeither chain is inherently better; their value depends on the reader's objectives."
          },
          "metadata": {},
          "execution_count": 605
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra credit"
      ],
      "metadata": {
        "id": "os-MM-ARKIic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore some use cases of LLMs beyond summarization and comparison of texts.\n",
        "\n",
        "We'll use the LangChain framework to prompt OpenAI's GPT models to do some interesting tasks for us."
      ],
      "metadata": {
        "id": "R43_Lna3TEQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation tool"
      ],
      "metadata": {
        "id": "Jxq1V2bXSRD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspiration: https://python.langchain.com/docs/tutorials/llm_chain/"
      ],
      "metadata": {
        "id": "beVCGgJJCpov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "dRD4us-gh5RA"
      },
      "execution_count": 608,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_4o_conservative = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", api_key=open_ai_key)"
      ],
      "metadata": {
        "id": "JQMAYlC02bze"
      },
      "execution_count": 609,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"\"\"Translate the following from English to {language}.\n",
        "                  Do not be verbose. Try to do an almost word for word translation while preserving the meaning.\n",
        "                  If the script is not roman, it is absolutely essential for you to provide the roman transliteration in parenthesis after the translation in the native script.\n",
        "                  Output the name of the language in boldface.\n",
        "                  \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "D98XBJxuSIxy"
      },
      "execution_count": 610,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What are you doing?\"\n",
        "language = \"Nepali, Hindi, Mandarin, Korean, Japanese, Spanish, Italian, Greek, Norwegian, Swahili\""
      ],
      "metadata": {
        "id": "MlZLDsMoihRK"
      },
      "execution_count": 611,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = prompt_template.invoke({\"language\": language, \"text\": text})"
      ],
      "metadata": {
        "id": "AdcKvqU9h9AE"
      },
      "execution_count": 612,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response = llm_4o_conservative.invoke(prompt)"
      ],
      "metadata": {
        "id": "mT_i0mOciPO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19fa3c18-b1b7-4a54-fe15-3bf5f23c1314"
      },
      "execution_count": 613,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 34.9 ms, sys: 2.31 ms, total: 37.2 ms\n",
            "Wall time: 2.93 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "eUPeI5TpiWB-",
        "outputId": "e03fb543-f306-4ffe-99ad-596353d9d1f3"
      },
      "execution_count": 614,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Nepali:** के गर्दैछौ? (ke gardai chhau?)\n\n**Hindi:** तुम क्या कर रहे हो? (tum kya kar rahe ho?)\n\n**Mandarin:** 你在做什么？ (nǐ zài zuò shénme?)\n\n**Korean:** 뭐 하고 있어요? (mwo hago isseoyo?)\n\n**Japanese:** 何をしていますか？ (nani o shiteimasu ka?)\n\n**Spanish:** ¿Qué estás haciendo?\n\n**Italian:** Cosa stai facendo?\n\n**Greek:** Τι κάνεις; (Ti káneis?)\n\n**Norwegian:** Hva gjør du?\n\n**Swahili:** Unafanya nini?"
          },
          "metadata": {},
          "execution_count": 614
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pet name generator?"
      ],
      "metadata": {
        "id": "3YokPEKCkbua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspiration: https://www.youtube.com/watch?v=lG7Uxts9SXs"
      ],
      "metadata": {
        "id": "UCE69yhkDKYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "creative_llm = ChatOpenAI(temperature=1, model = \"gpt-4-turbo-preview\", openai_api_key=open_ai_key)"
      ],
      "metadata": {
        "id": "7clK7YCZi6Fm"
      },
      "execution_count": 615,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys_temp_animal = \"\"\"You are someone who comes up with creative names.\n",
        "          I will give you the animal type, personality, and color of my pet.\n",
        "          I want you to suggest me five cool names for my pet.\n",
        "          Use one word names only.\n",
        "          \"\"\"\n",
        "\n",
        "prompt_temp_animal = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", sys_temp_animal), (\"user\", \"{animal_info}\")]\n",
        ")"
      ],
      "metadata": {
        "id": "zj8YmfJ08rzE"
      },
      "execution_count": 617,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animal_type = \"cat\"\n",
        "pet_color = \"brown\"\n",
        "personality = \"happy\"\n",
        "animal_info = f\"I have a {animal_type} with a {personality} personality, and it is {pet_color} in color.\""
      ],
      "metadata": {
        "id": "HAEcU7eI8jUI"
      },
      "execution_count": 618,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_animal = prompt_temp_animal.invoke({\"animal_info\": animal_info})"
      ],
      "metadata": {
        "id": "-AqxnNMN-JH9"
      },
      "execution_count": 619,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response_animal = creative_llm.invoke(prompt_animal)"
      ],
      "metadata": {
        "id": "rdfKupjM_Kxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbf0ae3-2b50-4a2d-9567-bd25e5e44d0d"
      },
      "execution_count": 620,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 21.1 ms, sys: 0 ns, total: 21.1 ms\n",
            "Wall time: 850 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response_animal.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "WIjq4JqS_P6A",
        "outputId": "4df1ed87-2eff-4a8c-974d-0f8323db4aba"
      },
      "execution_count": 621,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. Mocha\n2. Ember\n3. Hazel\n4. Sunny\n5. Copper"
          },
          "metadata": {},
          "execution_count": 621
        }
      ]
    }
  ]
}

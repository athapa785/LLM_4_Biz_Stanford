{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eMLQa5bBN9jXzJf293wlNsht2NUP47yv",
      "authorship_tag": "ABX9TyOofVHLAWQZWgLlkt+l4gQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athapa785/LLM_4_Biz_Stanford/blob/main/aditya_thapa_llm4biz_homework_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-F7mO0OU4yJ"
      },
      "outputs": [],
      "source": [
        "# Initialize key and client\n",
        "\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "\n",
        "client = OpenAI(api_key=open_ai_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  # response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an AI that takes instructions from a human and produces an answer. Be concise in your output.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a haiku about students studing large language models in Stanford on a Wednesday night.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "oV9PQxT9VFXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29891f52-5dcd-4168-f108-1471417d1ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-Avwht1xKwl6eCYivm8u0MkAvC4HN2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='In Stanford by night,  \\nStudents delve into the code,  \\nLanguage models shine.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738373729, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=50, total_tokens=68, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzvLRLY6JgkS",
        "outputId": "d066cf11-2ef4-4c6b-eeaa-cd356b3140d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Stanford by night,  \n",
            "Students delve into the code,  \n",
            "Language models shine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Homework"
      ],
      "metadata": {
        "id": "H5nGuNksZutC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Comparing texts from PDFs, webpages, or user input text.\n"
      ],
      "metadata": {
        "id": "yloseoYaxhRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests\n",
        "!pip install PyPDF2\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUhqeZChRDql",
        "outputId": "fe050b1c-18c8-4478-fd15-1de5aee3d631"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from  bs4 import BeautifulSoup\n",
        "import requests\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "t4eilXheqQap"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(sys_message, user_message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"{sys_message}\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{user_message}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only"
      ],
      "metadata": {
        "id": "O_xTCUPTKksA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_webpage_text(url):\n",
        "  \"\"\"\n",
        "  Extracts text from a webpage.\n",
        "\n",
        "  Parameters:\n",
        "  url (str): The URL of the webpage.\n",
        "\n",
        "  Returns:\n",
        "  str: The extracted text from the webpage.\n",
        "  \"\"\"\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  text = soup.get_text()\n",
        "  lines = (line.strip() for line in text.splitlines())\n",
        "  chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "  text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "  return text"
      ],
      "metadata": {
        "id": "MB9tu8ObMG11"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pdf_text(pdf_path):\n",
        "  \"\"\"\n",
        "  Extracts text from a PDF file.\n",
        "\n",
        "  Parameters:\n",
        "  pdf_path (str): The path to the PDF file.\n",
        "\n",
        "  Returns:\n",
        "  str: The extracted text from the PDF.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(pdf_path, 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF.PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "      page = pdf_reader.pages[page_num]\n",
        "      text += page.extract_text()\n",
        "      lines = (line.strip() for line in text.splitlines())\n",
        "      chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "      text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "    return text"
      ],
      "metadata": {
        "id": "IW7emMsSMFAf"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string, encoding_name=\"cl100k_base\"):\n",
        "  \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "  encoding = tiktoken.get_encoding(encoding_name)\n",
        "  num_tokens = len(encoding.encode(string))\n",
        "  return num_tokens"
      ],
      "metadata": {
        "id": "zE7roya2LY5A"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, max_tokens=2048):\n",
        "  \"\"\"\n",
        "  Summarizes a given text using OpenAI GPT-3.5, limiting the input text size.\n",
        "\n",
        "  Parameters:\n",
        "  text (str): The text to be summarized.\n",
        "  max_tokens (int): The maximum number of tokens allowed for the input text.\n",
        "\n",
        "  Returns:\n",
        "  str: The summarized text.\n",
        "  \"\"\"\n",
        "  # Limit the input text size to prevent exceeding token limits\n",
        "  text_tokens = num_tokens_from_string(text)\n",
        "  if text_tokens > max_tokens:\n",
        "    text = text[:max_tokens * 4] # Assume roughly 4 chars per token\n",
        "\n",
        "  sys_message = f\"You are an AI that takes instructions from a human and produces an answer. Be concise in your output.\"\n",
        "  user_message = f\"Summarize this text: {text}\"\n",
        "  return chat(sys_message, user_message)"
      ],
      "metadata": {
        "id": "IWUh8vYcrWBr"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_texts(*args):\n",
        "  \"\"\"\n",
        "  Compares two pieces of texts using OpenAI GPT-3.5.\n",
        "\n",
        "  Parameters:\n",
        "  text1, text2 (str): Texts to be compared\n",
        "\n",
        "  \"\"\"\n",
        "  summary = []\n",
        "  i = 1\n",
        "\n",
        "  for arg in args:\n",
        "    summary.append(summarize(arg))\n",
        "\n",
        "  texts = \"\"\n",
        "  for text in summary:\n",
        "    texts += f\"({i})\" + text + \";\"\n",
        "\n",
        "  user_message = f\"Compare {texts} for important differences\"\n",
        "\n",
        "  sys_message = f\"You are an AI that takes instructions from a human and produces an answer. Be analytical.\"\n",
        "  return chat(sys_message, user_message)"
      ],
      "metadata": {
        "id": "hYfPAa7xa1B9"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's try out an example."
      ],
      "metadata": {
        "id": "6dnNirlD2g-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = extract_webpage_text(\"https://drlee.io/step-by-step-guide-bayesian-optimization-with-random-forest-fdc6f329db9c\")\n",
        "text2 = extract_pdf_text(\"waves_quantum.pdf\")\n",
        "text3 = \"Quantum computing is going to be the next big thing after LLMs.\""
      ],
      "metadata": {
        "id": "ZzKa3VMXnjsY"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_texts(text1, text2, text3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "58pXV2VLqunx",
        "outputId": "5c8bc710-5713-4c82-e9ac-f4ad99bb7238"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The first text focuses on Bayesian Optimization as a method to enhance a Random Forest model's performance on a wine quality dataset through hyperparameter tuning, while the second text introduces quantum mechanics, discussing particle behavior based on the Schrodinger equation and key historical figures such as Planck, Einstein, Bohr, and de Broglie. The third text briefly mentions the emergence of quantum computing after LLMs as a significant advancement.\\n\\nOne important difference is the subject matter: the first text relates to machine learning and optimization techniques for model improvement, the second text delves into quantum mechanics and its historical background, and the third text highlights the potential of quantum computing in comparison to large language models (LLMs).\\n\\nAnother distinction lies in the focus and application domains. The first text is practical, aiming to enhance model performance on a specific dataset, the second text is theoretical, discussing fundamental physics principles, and the third text is forward-looking, discussing the potential future impact of quantum computing.\\n\\nFurthermore, the level of complexity varies across the texts. The first and third texts are more application-oriented and accessible to a wider audience, while the second text, introducing quantum mechanics concepts and historical developments, may require a deeper understanding of physics principles.\\n\\nIn summary, these texts differ in terms of subject matter, focus, application domains, complexity, and time perspective, showcasing diverse areas of study and technological advancements.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    }
  ]
}